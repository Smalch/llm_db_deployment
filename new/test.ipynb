{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:06:59.221482200Z",
     "start_time": "2023-06-13T23:06:55.138152100Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0 True\n",
      "Using device: 0 True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from pipeline.chain import Chain\n",
    "from pipeline.model import Model\n",
    "from pipeline.db import Db\n",
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "\n",
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import re\n",
    "import langchain\n",
    "from langchain import OpenAI, Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from typing import List, Dict, Callable\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import transformers\n",
    "import uvicorn\n",
    "device = 0 if torch.cuda.is_available() else -1  # set to GPU if available\n",
    "from langchain import OpenAI, Wikipedia\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents.react.base import DocstoreExplorer\n",
    "print(f\"Using device: {device}\", torch.cuda.is_available())\n",
    "device = 'auto'\n",
    "langchain.debug = True\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "class CustomPromptTemplate(StringPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        return self.template.format(**kwargs)\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import textwrap\n",
    "import torch\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        #regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        #match = re.search(regex, llm_output, re.DOTALL)\n",
    "\n",
    "        # if not match:\n",
    "        #     raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        # action = match.group(1).strip()\n",
    "        # action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        # answer = AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "        action, tool_input = llm_output.split(\"\\nAction: \")[1].strip().replace(']', '').split('[')\n",
    "        answer = AgentAction(tool=action, tool_input=tool_input, log=llm_output)\n",
    "        return answer\n",
    "\n",
    "\n",
    "\n",
    "def search_run(input: str) -> str:\n",
    "    output = search.run(input)\n",
    "    #answer = qa_chain.search(f'summarize the next text with respect to next request: {input}: {output}')\n",
    "    return output\n",
    "\n",
    "\n",
    "def docs_search(input: str) -> str:\n",
    "    output = qa_chain.search(input)\n",
    "    #output = Chain.process_llm_response(output, False)\n",
    "    return output\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Search',\n",
    "        func=search_run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "\n",
    "    Tool(\n",
    "        name='Lookup',\n",
    "        func=docs_search,\n",
    "        description=\"useful for when you need to answer questions about specific precise information in documents\"\n",
    "\n",
    "    )\n",
    "]\n",
    "output_parser = CustomOutputParser()\n",
    "path = './Training_materials/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:08:54.562345500Z",
     "start_time": "2023-06-13T23:06:59.192671200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005bb834375d4b3eaac02db59516d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/e5-large-v2. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6845c183e6472dacffa132e5c73db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "qa_chain = Chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:08:54.562345500Z",
     "start_time": "2023-06-13T23:08:54.561081800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:08:54.562345500Z",
     "start_time": "2023-06-13T23:08:54.562345500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:38.438445500Z",
     "start_time": "2023-06-13T23:09:30.142461600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda56390de914dc4902166c52c35aec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    'models/mpt-7b-storywriter',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=device,\n",
    "    max_length=83968\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:38.633228200Z",
     "start_time": "2023-06-13T23:09:38.446591300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:38.693115500Z",
     "start_time": "2023-06-13T23:09:38.661453100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:38.693115500Z",
     "start_time": "2023-06-13T23:09:38.661453100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# mtp-7b is trained to add \"<|endoftext|>\" at the end of generations\n",
    "stop_token_ids = tokenizer.convert_tokens_to_ids([\"<|endoftext|>\"])\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in stop_token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:38.693115500Z",
     "start_time": "2023-06-13T23:09:38.661453100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:40.671502200Z",
     "start_time": "2023-06-13T23:09:40.671502200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'MPTForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    device_map=device,\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model will ramble\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    top_p=0.15,  # select from top tokens whose probability add up to 15%\n",
    "    top_k=0,  # select from top 0 tokens (because zero, relies on top_p)\n",
    "    max_new_tokens=1000,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:41.620219800Z",
     "start_time": "2023-06-13T23:09:41.585360600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe_mpt = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T15:33:19.902067200Z",
     "start_time": "2023-06-13T15:32:10.584231200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpt/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Tell about Vantage Towers financial situation?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Tell about Vantage Towers financial situation?\",\n",
      "  \"context\": \"5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\\n\\n5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\\n\\n5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\\n\\n5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\\n\\n5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 15/21CapEx per square meter (up to a factor of x0.8\\ncompared to small sites). In any case, Cellnex\\nproposes different technological solutions to\\nmeet customer expectations at the same time\\nthat minimises the cost.\\nUser Experience: Moreover, venues will benefit\\nfrom a better client/user experience with a unique\\nintermediary.\\nOperational Management:  At the operational\\nlevel, infrastructure management of an\\nintermediary facilitates and brings a differential\\nadded value for the venue in lead time.\\nThus, the cost of a DAS system varies depending on the\\nvariables mentioned above. Cellnex model works in a\\npersonalised way for every project, which allows costs\\nminimisation while fulfilling all the MNOs’ needs. For\\nbuildings, in most of the cases, CapEx of multi-operator\\nsolutions ranges between 2,5 € / m² to 12,00 € /m².\\nTherefore, the costs of a DAS solution are highly\\ndependent on some factors and requirements, and also\\ndepends on the kind of technology to be used. In any\\ncase,  considerable efficiencies using shared DAS\\ninfrastructure can be achieved.\\nPlease login above to view OpEx/CapEx saving figures.\\n5. Wanda Metropolitano Project\\nStadiums are an excellent example of the data explosion\\nphenomena and the adoption of new apps and\\nfunctionalities by the user in the mobility environment.\\nFor instance, during the last Superbowl final, 55 TB of\\ndata was consumed over the mobile network. In recent\\nyears we have seen growth around 100% in the\\nconsumption of data traffic in stadiums. Social media is\\nhaving a significant impact on sports. Today, fans use\\ntheir smartphones to share their fan experience with the\\nrest of the world. It has been reported that the top apps\\n\\nQuestion: Tell about Vantage Towers financial situation?\\nHelpful Answer:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:HuggingFacePipeline] [69.28s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Vantage Towers is a European telecommunications infrastructure company headquartered in Barcelona, Spain. The company was formed in 2019 through the merger of Cellnex Telecom and Abertis Telecom. As of 2021, it operates more than 70,000 telecom towers across Europe, including Spain, Italy, France, Belgium, Ireland, Switzerland, and the Netherlands. The company provides services such as mobile telephony, fixed telephony, fiber optic networks, and broadcasting. Its revenue in 2020 was approximately €3 billion.\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [69.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \" Vantage Towers is a European telecommunications infrastructure company headquartered in Barcelona, Spain. The company was formed in 2019 through the merger of Cellnex Telecom and Abertis Telecom. As of 2021, it operates more than 70,000 telecom towers across Europe, including Spain, Italy, France, Belgium, Ireland, Switzerland, and the Netherlands. The company provides services such as mobile telephony, fixed telephony, fiber optic networks, and broadcasting. Its revenue in 2020 was approximately €3 billion.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:StuffDocumentsChain] [69.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \" Vantage Towers is a European telecommunications infrastructure company headquartered in Barcelona, Spain. The company was formed in 2019 through the merger of Cellnex Telecom and Abertis Telecom. As of 2021, it operates more than 70,000 telecom towers across Europe, including Spain, Italy, France, Belgium, Ireland, Switzerland, and the Netherlands. The company provides services such as mobile telephony, fixed telephony, fiber optic networks, and broadcasting. Its revenue in 2020 was approximately €3 billion.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [69.31s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Vantage Towers is a European telecommunications infrastructure company headquartered in Barcelona, Spain. The\\ncompany was formed in 2019 through the merger of Cellnex Telecom and Abertis Telecom. As of 2021, it operates\\nmore than 70,000 telecom towers across Europe, including Spain, Italy, France, Belgium, Ireland, Switzerland,\\nand the Netherlands. The company provides services such as mobile telephony, fixed telephony, fiber optic\\nnetworks, and broadcasting. Its revenue in 2020 was approximately €3 billion.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_mpt = Model('mpt-7b-storywriter', temperature=0, max_length=2000).llm\n",
    "# llm_mpt(\"How can I treat a sprained ankle?\")\n",
    "\n",
    "\n",
    "docs_search('Tell me about Vantage Towers financial situation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T16:13:38.846750100Z",
     "start_time": "2023-06-13T16:12:22.145512800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba0a8dfef1c4fdc8c26bc22fd321df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T21:24:29.908558600Z",
     "start_time": "2023-06-13T21:23:37.460131Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2681ad2a0ee149b78108e7d98dc5b0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RWForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "model= Model()\n",
    "llm = model.llm1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:09:46.421119600Z",
     "start_time": "2023-06-13T23:09:46.411491Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_generator = LLMChain(llm=pipe_mpt, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "doc_chain = load_qa_with_sources_chain(pipe_mpt, chain_type=\"map_reduce\")\n",
    "\n",
    "cr_chain = ConversationalRetrievalChain(\n",
    "    retriever=qa_chain.retriever,\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:13:07.801145700Z",
     "start_time": "2023-06-13T23:09:46.691406500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"What is MNO?\",\n",
      "  \"chat_history\": []\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input_list\": [\n",
      "    {\n",
      "      \"context\": \"MNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\",\n",
      "      \"question\": \"What is MNO?\",\n",
      "      \"chat_history\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"MNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\",\n",
      "      \"question\": \"What is MNO?\",\n",
      "      \"chat_history\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"context\": \"MNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\",\n",
      "      \"question\": \"What is MNO?\",\n",
      "      \"chat_history\": \"\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain > 3:chain:LLMChain > 4:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nMNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\\nQuestion: What is MNO?\\nRelevant text, if any:\",\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nMNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\\nQuestion: What is MNO?\\nRelevant text, if any:\",\n",
      "    \"Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\nMNO  Mobile  Network Operator  \\nMNO PSU:  MNOs Power Supply Unit(s) for BTS and NTE  per MNO  \\nMT Mobile Terminal  \\nNDA  Non -Disclosure Agreement  \\nNTE:  Network Transmission Equipment for connecting the traffic to the core  \\n network, (can be more than one)  per MNO  \\n \\n2 The NEN - EN 50173 and 50174 also defines locations which can be applicable. For example MER is \\ncalled Building Distribution (DB)  and SER is called Floor Distribution (FD).\\nQuestion: What is MNO?\\nRelevant text, if any:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpt/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain > 3:chain:LLMChain > 4:llm:HuggingFacePipeline] [196.89s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ],\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain > 3:chain:LLMChain] [196.89s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"\\n\\n   * 2.1 General \\n    *...\\n      * A mobile terminal shall have at least two power supply units connected in parallel with each other or in series depending on the type of equipment used by the operator; these are referred as \\\"mobile terminals\\\" hereon after...\\n        * 3.3 Network transmission equipments [NTE] may consist of several parts such as base transceiver station [BTS], radio access node [RAN], etc., but they all must comply with this standard....\\n     * 4.4 In case of multiple connections between different networks it should not matter where the connection takes place provided that there exists an interface defined within this standard. This means that when using interfaces like GSM-A/GPRS-C, Iu-PSU, E-UTRAN-EPC, UTRAN-GERAN, TETRA-TCCH, WCDMA-UMTS, CDMA2000-XSC, TD-SCDMA-FACH, WiMAX-WiFi, HSDPA, HSPAP, SIP, Diameter, XMPP, OMA, IPsec, SS7, ISUP, SIGTRAN, ATM, Ethernet, Frame Relay, SDH, DVB-SHD, DOCSIS, xDSl, VON, VoLTE, RTTY, SMS, USIM, SIM, eSIM, CNG, FTP, RTSP, TCP, UDP, ICMP, SNPP, DHCP, DNS, ARP, RADIUS, SAP, MGCP, DIAMETER, LDP, RSVP, QSIG, STUN, ICE, NAT, GRE, VPN, VLAN, VOIP, ALTO, AMBR, PROTOCOL, TR069, ZRTP, WebSocket, JPEG, MP3, AVIF, ASK, VP8, WEBM, FLV, SVG, WMV, MKV, TS, MSO, VC-1, ACELP, ADPCM, PCMU, PMTU, PDCP, PANA, EVRC, HE-AAC, HDMI, USB, Bluetooth, WLAN, WiFi, 802.11a, b, g, n, ac, d, h, i, k, r, s, u, v, w, z, y, p, q, t, c, l, o, m, f, j,,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y,x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,c,v,w,z,y, x,b,d,e,f,g,h,j,k,m,n,o,p,q,r,t,\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4680 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2568 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3624 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain > 2:chain:MapReduceDocumentsChain] [200.87s] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('A single document was so long it could not be combined with another document, we cannot handle this.')\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:ConversationalRetrievalChain] [200.90s] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('A single document was so long it could not be combined with another document, we cannot handle this.')\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A single document was so long it could not be combined with another document, we cannot handle this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is MNO?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcr_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:114\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    112\u001b[0m new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[1;32m    113\u001b[0m new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[0;32m--> 114\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_inputs\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m output: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer}\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:261\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:149\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mCombine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03mThis reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# FYI - this is parallelized and so it is fast.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_variable_name: d\u001b[38;5;241m.\u001b[39mpage_content, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs} \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs],\n\u001b[1;32m    147\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:191\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain._process_results\u001b[0;34m(self, results, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collapse_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    187\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_tokens \u001b[38;5;241m>\u001b[39m token_max:\n\u001b[0;32m--> 191\u001b[0m     new_result_doc_list \u001b[38;5;241m=\u001b[39m \u001b[43m_split_list_of_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m docs \u001b[38;5;129;01min\u001b[39;00m new_result_doc_list:\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:37\u001b[0m, in \u001b[0;36m_split_list_of_docs\u001b[0;34m(docs, length_func, token_max, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single document was longer than the context length,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m we cannot handle this.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_sub_result_docs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single document was so long it could not be combined \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith another document, we cannot handle this.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m new_result_doc_list\u001b[38;5;241m.\u001b[39mappend(_sub_result_docs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     42\u001b[0m _sub_result_docs \u001b[38;5;241m=\u001b[39m _sub_result_docs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mValueError\u001b[0m: A single document was so long it could not be combined with another document, we cannot handle this."
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What is MNO?\"\n",
    "result = cr_chain({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T23:13:07.769178100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:07:26.929164400Z",
     "start_time": "2023-06-13T22:07:26.915179Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapReduceDocumentsChain(lc_kwargs={'llm_chain': LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs={'template': 'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', 'input_variables': ['context', 'question']}, input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs={'template': 'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', 'input_variables': ['context', 'question']}, input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), 'combine_document_chain': StuffDocumentsChain(lc_kwargs={'llm_chain': LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), 'document_variable_name': 'summaries', 'document_prompt': PromptTemplate(lc_kwargs={'template': 'Content: {page_content}\\nSource: {source}', 'input_variables': ['page_content', 'source']}, input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), document_prompt=PromptTemplate(lc_kwargs={'template': 'Content: {page_content}\\nSource: {source}', 'input_variables': ['page_content', 'source']}, input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), document_variable_name='summaries', document_separator='\\n\\n'), 'document_variable_name': 'context', 'collapse_document_chain': None, 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs={'template': 'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', 'input_variables': ['context', 'question']}, input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs={'template': 'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', 'input_variables': ['context', 'question']}, input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), combine_document_chain=StuffDocumentsChain(lc_kwargs={'llm_chain': LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), 'document_variable_name': 'summaries', 'document_prompt': PromptTemplate(lc_kwargs={'template': 'Content: {page_content}\\nSource: {source}', 'input_variables': ['page_content', 'source']}, input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(lc_kwargs={'llm': HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), 'prompt': PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), 'verbose': None}, memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(lc_kwargs='{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}', input_variables=['summaries', 'question'], output_parser=None, partial_variables={}, template='rtgrfdfffdgfsdfd', template_format='f-string', validate_template=True), llm=HuggingFacePipeline(lc_kwargs={'pipeline': <transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>}, cache=None, verbose=False, callbacks=None, callback_manager=None, pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7f4de54375b0>, model_id='gpt2', model_kwargs=None, pipeline_kwargs=None), output_key='text'), document_prompt=PromptTemplate(lc_kwargs={'template': 'Content: {page_content}\\nSource: {source}', 'input_variables': ['page_content', 'source']}, input_variables=['page_content', 'source'], output_parser=None, partial_variables={}, template='Content: {page_content}\\nSource: {source}', template_format='f-string', validate_template=True), document_variable_name='summaries', document_separator='\\n\\n'), collapse_document_chain=None, document_variable_name='context', return_intermediate_steps=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain = load_qa_with_sources_chain(qa_chain.llm, chain_type=\"map_reduce\")\n",
    "doc_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:09:37.051216100Z",
     "start_time": "2023-06-13T22:09:37.028286100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(lc_kwargs={'template': 'Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', 'input_variables': ['context', 'question']}, input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='Use the following portion of a long document to see if any of the text is relevant to answer the question. \\nReturn any relevant text verbatim.\\n{context}\\nQuestion: {question}\\nRelevant text, if any:', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.qa_with_sources.map_reduce_prompt import (\n",
    "    COMBINE_PROMPT,\n",
    "    EXAMPLE_PROMPT,\n",
    "    QUESTION_PROMPT,\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:07:40.520525900Z",
     "start_time": "2023-06-13T22:07:40.519474800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtgrfdfffdgfsdfd'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain.combine_document_chain.lc_kwargs['llm_chain'].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:05:16.829786900Z",
     "start_time": "2023-06-13T22:05:16.813844200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_chain.combine_document_chain.lc_kwargs['llm_chain'].prompt.lc_kwargs = '''{'template': 'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:','input_variables': ['summaries', 'question']}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T22:06:42.481149500Z",
     "start_time": "2023-06-13T22:06:42.476087100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'template\\': \\'Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\"). \\nIf you don\\'t know the answer, just say that you don\\'t know. Don\\'t try to make up an answer.\\nALWAYS return a \"SOURCES\" part in your answer.\\n\\nQUESTION: {question}\\n=========\\n{summaries}\\n=========\\nFINAL ANSWER:\\',\\'input_variables\\': [\\'summaries\\', \\'question\\']}'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chain.combine_document_chain.lc_kwargs['llm_chain'].prompt.lc_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:05:29.163815500Z",
     "start_time": "2023-06-13T14:05:25.201989Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/e5-large-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'e5-large-v2'\n",
    "model_path = f'./models/{model_name}'\n",
    "model_kwargs = {\n",
    "    'device': 'cuda:0'\n",
    "}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_path, model_kwargs=model_kwargs)\n",
    "vectorDB = Chroma(persist_directory='./db', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:05:29.176861Z",
     "start_time": "2023-06-13T14:05:29.168902400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7ff5b718dea0>, search_type='similarity', search_kwargs={'k': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorDB.as_retriever(search_kwargs={'k': 2})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:05:32.193041Z",
     "start_time": "2023-06-13T14:05:29.179768300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents('Vantage Towers?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T12:25:59.673406700Z",
     "start_time": "2023-06-13T12:25:59.664660900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x7f9db88f14e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.db.vectorDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:13:05.402299400Z",
     "start_time": "2023-06-13T13:13:02.530351600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ./models/e5-large-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "db = Db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:04:33.707922800Z",
     "start_time": "2023-06-13T13:03:45.239446800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.reload_docs_with()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T13:13:20.247209700Z",
     "start_time": "2023-06-13T13:13:19.853442Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.retriever.get_relevant_documents('Vantage Towers?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:08:43.573304500Z",
     "start_time": "2023-06-13T10:08:43.557358800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "react = initialize_agent(tools, qa_chain.llm, agent=AgentType.REACT_DOCSTORE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T11:04:34.607864400Z",
     "start_time": "2023-06-13T11:04:34.561020300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(lc_kwargs={'page_content': '5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 7/21stadiums, shopping centres, and skyscrapers. On the road\\nto 5G, DAS solutions are continuously evolving to add\\nnew functionalities, improve cost-efficiency, reduce\\nfootprint and adapt to C-RAN and V-RAN typologies.\\nEnd-to-end digital solutions with IT structured cabling is\\non the rise. Beyond Passive and Active DAS solutions, a\\nnew Advanced DAS  is emerging in the market. This\\nsolution is also called DRS, C-RAN DAS, or even assumed\\nto be Small Cells by some third parties. Advanced DAS is\\nvery low-power solutions that open the door to advanced\\nfeatures such as improved positioning by distributing\\nradio functionalities to the antenna. There’s also a shift\\ntowards C-RAN (Centralised RAN) architecture, where\\ncentralised baseband functionality is shared across a\\nlarge number of distributed radio nodes to deliver', 'metadata': {'source': 'Training_materials/GSMA _ Case Study_ Cellnex - Future Networks.pdf', 'page': 6}}, page_content='5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 7/21stadiums, shopping centres, and skyscrapers. On the road\\nto 5G, DAS solutions are continuously evolving to add\\nnew functionalities, improve cost-efficiency, reduce\\nfootprint and adapt to C-RAN and V-RAN typologies.\\nEnd-to-end digital solutions with IT structured cabling is\\non the rise. Beyond Passive and Active DAS solutions, a\\nnew Advanced DAS  is emerging in the market. This\\nsolution is also called DRS, C-RAN DAS, or even assumed\\nto be Small Cells by some third parties. Advanced DAS is\\nvery low-power solutions that open the door to advanced\\nfeatures such as improved positioning by distributing\\nradio functionalities to the antenna. There’s also a shift\\ntowards C-RAN (Centralised RAN) architecture, where\\ncentralised baseband functionality is shared across a\\nlarge number of distributed radio nodes to deliver', metadata={'source': 'Training_materials/GSMA _ Case Study_ Cellnex - Future Networks.pdf', 'page': 6}),\n",
       " Document(lc_kwargs={'page_content': '5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 14/21its needs in time without being bound to a short-term\\ncontract, driving the economic benefits for the MNO.\\nThe model assumed by Cellnex reverts in a cost reduction\\nfor the MNO and a transparent service for the landlord,\\nbecoming an optimal solution for all involved parties.\\nCellnex’s vast expertise in DAS services has shown us\\nthat the cost of a DAS system deployment varies\\ndepending on the following factors:\\nLocation and environment:  Macro sites\\ninterferences from outside venues will affect the\\ncoverage inside the venue. In extremely interfered\\nplaces, it will be required reinforcement of the\\ninternal radio signal meaning extra CapEx (up to a\\nfactor of x2), while in cases of total isolation the\\nsame equipment will give a better signal\\nperformance.\\nPerformance:  Expected technical performance,\\nuser experience, and future-proofing drive to the\\nmost suitable technology and approach to fulfil\\nexpectations. Parameters such as diversity (SISO,\\nMIMOx2, MIMOx4), number of frequency bands or\\nKPI related to signal quality and strength of the\\nsignal will have a strong influence on the kind of\\nsystem to deploy. A top-performance system\\nmight make the CapEx differ up to a factor of x2.5\\ncompared to very basic performing system.\\nType of venue:  The kind of building will influence\\nthe engineering of the solution; inner walls, types\\nof construction or materials define the indoor\\nradio structure. Constructions with thick inner\\nwalls and/or small rooms induce to improve the\\ncoverage by adding more radio equipment. On\\nthe other hand, venues with wide-open spaces\\nfacilitate the coverage and can reduce the CapEx\\nup to half.\\nArea to cover:  Considering the same kind of\\nsolution, large venues usually benefit from a lower', 'metadata': {'source': 'Training_materials/GSMA _ Case Study_ Cellnex - Future Networks.pdf', 'page': 13}}, page_content='5/5/2020 GSMA | Case Study: Cellnex - Future Networks\\nhttps://www .gsma.com/futurenetworks/wiki/case-study-das-as-a-service/ 14/21its needs in time without being bound to a short-term\\ncontract, driving the economic benefits for the MNO.\\nThe model assumed by Cellnex reverts in a cost reduction\\nfor the MNO and a transparent service for the landlord,\\nbecoming an optimal solution for all involved parties.\\nCellnex’s vast expertise in DAS services has shown us\\nthat the cost of a DAS system deployment varies\\ndepending on the following factors:\\nLocation and environment:  Macro sites\\ninterferences from outside venues will affect the\\ncoverage inside the venue. In extremely interfered\\nplaces, it will be required reinforcement of the\\ninternal radio signal meaning extra CapEx (up to a\\nfactor of x2), while in cases of total isolation the\\nsame equipment will give a better signal\\nperformance.\\nPerformance:  Expected technical performance,\\nuser experience, and future-proofing drive to the\\nmost suitable technology and approach to fulfil\\nexpectations. Parameters such as diversity (SISO,\\nMIMOx2, MIMOx4), number of frequency bands or\\nKPI related to signal quality and strength of the\\nsignal will have a strong influence on the kind of\\nsystem to deploy. A top-performance system\\nmight make the CapEx differ up to a factor of x2.5\\ncompared to very basic performing system.\\nType of venue:  The kind of building will influence\\nthe engineering of the solution; inner walls, types\\nof construction or materials define the indoor\\nradio structure. Constructions with thick inner\\nwalls and/or small rooms induce to improve the\\ncoverage by adding more radio equipment. On\\nthe other hand, venues with wide-open spaces\\nfacilitate the coverage and can reduce the CapEx\\nup to half.\\nArea to cover:  Considering the same kind of\\nsolution, large venues usually benefit from a lower', metadata={'source': 'Training_materials/GSMA _ Case Study_ Cellnex - Future Networks.pdf', 'page': 13})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.db.retriever.get_relevant_documents(\"What is the most promising direction for investment for the Vantage Towers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#react.run(\"What is the most promising direction for investment for the Vantage Towers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './Training_materials/'\n",
    "qa_chain = Chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:52.537487200Z",
     "start_time": "2023-06-13T10:14:52.532504800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_run(input: str) -> str:\n",
    "    output = search.run(input)\n",
    "    answer = qa_chain.search(f'summarize the next text with respect to next request: {input}: {output}')\n",
    "    return answer\n",
    "\n",
    "def docs_search(input: str) -> str:\n",
    "    output = qa_chain.search(input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:53.266886300Z",
     "start_time": "2023-06-13T10:14:53.223121300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='search',\n",
    "        func=search_run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "\n",
    "    Tool(\n",
    "        name='lookup_in_docs',\n",
    "        func=docs_search,\n",
    "        description=\"useful for when you need to answer questions about specific precise information in documents\"\n",
    "\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:53.959776400Z",
     "start_time": "2023-06-13T10:14:53.942829500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\n",
    "#\n",
    "# {tools}\n",
    "#\n",
    "# Use the following format:\n",
    "#\n",
    "# Question: the input question you must answer\n",
    "# Thought: you should always think about what to do\n",
    "# Action: the action to take, should be one of [{tool_names}]\n",
    "# Action Input: the input to the action\n",
    "# Observation: the result of the action\n",
    "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n",
    "#\n",
    "# Begin! Remember to answer as a compasionate specialist in cell towers and all related fields when giving your final answer.\n",
    "#\n",
    "# Question: {input}\n",
    "# {agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:52:04.860470700Z",
     "start_time": "2023-06-13T10:52:04.857482500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:52:04.868449200Z",
     "start_time": "2023-06-13T10:52:04.868449200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prompt = CustomPromptTemplate(\n",
    "#     template=template,\n",
    "#     tools=tools,\n",
    "#     input_variables=[\"input\", \"intermediate_steps\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:52:04.893458600Z",
     "start_time": "2023-06-13T10:52:04.868449200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "\n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        #regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        #match = re.search(regex, llm_output, re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "        # if not match:\n",
    "        #     raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        # action = match.group(1).strip()\n",
    "        # action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        # answer = AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
    "\n",
    "        action, tool_input  = llm_output.split(\"\\nAction: \")[1].strip().replace(']','').split('[')\n",
    "        answer = AgentAction(tool=action, tool_input=tool_input, log=llm_output)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:52:04.893458600Z",
     "start_time": "2023-06-13T10:52:04.875424600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:55.254807400Z",
     "start_time": "2023-06-13T10:14:55.206968Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# llm = qa_chain.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:55.533598800Z",
     "start_time": "2023-06-13T10:14:55.491116100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:55.888032400Z",
     "start_time": "2023-06-13T10:14:55.885042300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tool_names = [tool.name for tool in tools]\n",
    "#\n",
    "# agent = LLMSingleActionAgent(\n",
    "#     llm_chain=llm_chain,\n",
    "#     output_parser=output_parser,\n",
    "#     stop=[\"\\nObservation:\"],\n",
    "#     allowed_tools=tool_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:14:56.171678300Z",
     "start_time": "2023-06-13T10:14:56.121787900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent_executor = AgentExecutor.from_agent_and_tools(agent=agent,\n",
    "#                                                     tools=tools,\n",
    "#                                                     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T10:31:19.642151700Z",
     "start_time": "2023-06-13T10:31:19.638118800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Search', 'Colorado orogeny']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Action: Search[Colorado orogeny]'.split(\"Action: \")[1].strip().replace(']','').split('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:13:21.222454Z",
     "start_time": "2023-06-13T14:13:21.208328400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_with_history = \"\"\"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "template_with_history = \"\"\"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}][the input to the action]\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "Thought: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "Action: Search[Colorado orogeny]\n",
    "Observation: The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "Thought: It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "Action: Lookup[eastern sector]\n",
    "Observation: (Result 1 / 1) The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "Thought: The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "Action: Search[High Plains]\n",
    "Observation: High Plains refers to one of two distinct land regions\n",
    "Thought: I need to instead search High Plains (United States).\n",
    "Action: Search[High Plains (United States)]\n",
    "Observation: The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130 m).[3]\n",
    "Thought: Final Answer: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "\n",
    "\n",
    "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "Thought: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "Action: Search[Milhouse]\n",
    "Observation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "Thought: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "Action: Lookup[named after]\n",
    "Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "Thought: Final Answer: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:13:18.345115500Z",
     "start_time": "2023-06-13T23:13:18.334168400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "template_with_history = \"\"\"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}][the input to the action]\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: Final Answer: the final answer to the original input question\n",
    "\n",
    "\n",
    "Question: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "Thought: The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "Action: Search[Milhouse]\n",
    "Observation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "Thought: The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "Action: Lookup[named after]\n",
    "Observation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "Thought: Final Answer: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Previous conversation history:\n",
    "{history}\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:13:18.845068900Z",
     "start_time": "2023-06-13T23:13:18.843469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_with_history = CustomPromptTemplate(\n",
    "    template=template_with_history,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T23:14:47.041331800Z",
     "start_time": "2023-06-13T23:13:19.472348500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the most promising direction for investment for the Vantage Towers?\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"intermediate_steps\": [],\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\"\n",
      "  ],\n",
      "  \"input\": \"What is the most promising direction for investment for the Vantage Towers?\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\\n\\nSearch: useful for when you need to answer questions about current events\\nLookup: useful for when you need to answer questions about specific precise information in documents\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Lookup][the input to the action]\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: Final Answer: the final answer to the original input question\\n\\n\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \\\"The Simpsons\\\" character Milhouse, who Matt Groening named after who?\\nThought: The question simplifies to \\\"The Simpsons\\\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\\nAction: Search[Milhouse]\\nObservation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\\nThought: The paragraph does not tell who Milhouse is named after, maybe I can look up \\\"named after\\\".\\nAction: Lookup[named after]\\nObservation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\\nThought: Final Answer: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\\n\\n\\nBegin!\\n\\nPrevious conversation history:\\n\\n\\nQuestion: What is the most promising direction for investment for the Vantage Towers?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] [87.25s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nThought: There are many directions that could lead us into profitability; however, we will focus on two main ones at this time—a new line of business with our existing clients or expanding our client base through acquisition. We already know from previous conversations that there has been some interest among certain companies regarding expansion opportunities within their own businesses. This would allow them to expand without having to invest any capital themselves while still increasing profits significantly. If these leads pan out successfully then they may become long-term customers which means more revenue over time. However if none of those options work out well enough than perhaps we might consider looking outside ourselves entirely instead like acquiring another company altogether rather than trying to grow organically inside ours first before branching off again later down the road once things settle back down somewhat further along afterwards depending upon how much money comes rolling in during that period afterward too though hopefully everything works itself out smoothly either way regardless since both scenarios seem equally viable right now anyway even though I'm leaning slightly towards organic growth myself personally just because I feel like it's less risky overall especially considering how volatile markets tend to get sometimes nowadays due to various factors such as global warming etcetera although I suppose it depends mostly on whether or not people want to buy products made here locally versus overseas somewhere else elsewhere across borders internationally worldwide everywhere around world Earth planet universe solar system galaxy Milky Way et cetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [87.25s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\nThought: There are many directions that could lead us into profitability; however, we will focus on two main ones at this time—a new line of business with our existing clients or expanding our client base through acquisition. We already know from previous conversations that there has been some interest among certain companies regarding expansion opportunities within their own businesses. This would allow them to expand without having to invest any capital themselves while still increasing profits significantly. If these leads pan out successfully then they may become long-term customers which means more revenue over time. However if none of those options work out well enough than perhaps we might consider looking outside ourselves entirely instead like acquiring another company altogether rather than trying to grow organically inside ours first before branching off again later down the road once things settle back down somewhat further along afterwards depending upon how much money comes rolling in during that period afterward too though hopefully everything works itself out smoothly either way regardless since both scenarios seem equally viable right now anyway even though I'm leaning slightly towards organic growth myself personally just because I feel like it's less risky overall especially considering how volatile markets tend to get sometimes nowadays due to various factors such as global warming etcetera although I suppose it depends mostly on whether or not people want to buy products made here locally versus overseas somewhere else elsewhere across borders internationally worldwide everywhere around world Earth planet universe solar system galaxy Milky Way et cetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera etcetera\"\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [87.26s] Chain run errored with error:\n",
      "\u001b[0m\"IndexError('list index out of range')\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferWindowMemory(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     13\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor\u001b[38;5;241m.\u001b[39mfrom_agent_and_tools(\n\u001b[1;32m     14\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m     15\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the most promising direction for investment for the Vantage Towers?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:258\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:957\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 957\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    966\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    967\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:345\u001b[0m, in \u001b[0;36mLLMSingleActionAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    340\u001b[0m     intermediate_steps\u001b[38;5;241m=\u001b[39mintermediate_steps,\n\u001b[1;32m    341\u001b[0m     stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop,\n\u001b[1;32m    342\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m )\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m, in \u001b[0;36mCustomOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentFinish(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# Return values is generally always a dictionary with a single `output` key\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# It is not recommended to try anything else at the moment :)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         return_values\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()},\n\u001b[1;32m     92\u001b[0m         log\u001b[38;5;241m=\u001b[39mllm_output,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Parse out the action and action input\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#match = re.search(regex, llm_output, re.DOTALL)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Return the action and action input\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# answer = AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m action, tool_input \u001b[38;5;241m=\u001b[39m \u001b[43mllm_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mAction: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m answer \u001b[38;5;241m=\u001b[39m AgentAction(tool\u001b[38;5;241m=\u001b[39maction, tool_input\u001b[38;5;241m=\u001b[39mtool_input, log\u001b[38;5;241m=\u001b[39mllm_output)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#llm_chain = LLMChain(llm=qa_chain.llm, prompt=prompt_with_history)\n",
    "llm_chain = LLMChain(llm=pipe_mpt, prompt=prompt_with_history)\n",
    "\n",
    "\n",
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    max_iterations=10\n",
    ")\n",
    "agent_executor.run(\"What is the most promising direction for investment for the Vantage Towers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:38.166713200Z",
     "start_time": "2023-06-13T14:17:38.067773400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:38.166713200Z",
     "start_time": "2023-06-13T14:17:38.083649700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#llm_mpt = Model('mpt-7b-storywriter', temperature=0, max_length=83968).llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:38.166713200Z",
     "start_time": "2023-06-13T14:17:38.083649700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:38.166713200Z",
     "start_time": "2023-06-13T14:17:38.083649700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:17:38.166713200Z",
     "start_time": "2023-06-13T14:17:38.083649700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:21:33.635441100Z",
     "start_time": "2023-06-13T14:21:33.619668400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=qa_chain.llm, prompt=prompt_with_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:21:33.747898100Z",
     "start_time": "2023-06-13T14:21:33.669120400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain,\n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"],\n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:21:33.747898100Z",
     "start_time": "2023-06-13T14:21:33.690804800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memory=ConversationBufferWindowMemory(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:21:33.747898100Z",
     "start_time": "2023-06-13T14:21:33.690804800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    "    max_iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T14:23:12.107421600Z",
     "start_time": "2023-06-13T14:21:33.690804800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gpt/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the most promising direction for investment for the Vantage Towers?\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"intermediate_steps\": [],\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\"\n",
      "  ],\n",
      "  \"input\": \"What is the most promising direction for investment for the Vantage Towers?\",\n",
      "  \"history\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\\n\\nSearch: useful for when you need to answer questions about current events\\nLookup: useful for when you need to answer questions about specific precise information in documents\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Lookup][the input to the action]\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: Final Answer: the final answer to the original input question\\n\\n\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \\\"The Simpsons\\\" character Milhouse, who Matt Groening named after who?\\nThought: The question simplifies to \\\"The Simpsons\\\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\\nAction: Search[Milhouse]\\nObservation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\\nThought: The paragraph does not tell who Milhouse is named after, maybe I can look up \\\"named after\\\".\\nAction: Lookup[named after]\\nObservation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\\nThought: Final Answer: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\\n\\n\\nBegin!\\n\\nPrevious conversation history:\\n\\n\\nQuestion: What is the most promising direction for investment for the Vantage Towers?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:HuggingFacePipeline] [56.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Thought: This question requires me to know the current state of the market and the company's financial situation. I will need to use my knowledge of the industry and research on the company's performance.\\nAction: Lookup[Vantage Towers]\",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [56.35s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Thought: This question requires me to know the current state of the market and the company's financial situation. I will need to use my knowledge of the industry and research on the company's performance.\\nAction: Lookup[Vantage Towers]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Lookup] Entering Tool run with input:\n",
      "\u001b[0m\"Vantage Towers\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Lookup] [30.949ms] Exiting Tool run with output:\n",
      "\u001b[0m\"[Document(lc_kwargs={'page_content': 'ware is unusual and generally not desirable because the credibility (although\\nnot necessarily the accuracy) of a model is proportional to how widelyaccepted it is.\\nREFERENCES\\n1. H. L. Bertoni, et al., UHF propagation prediction for wireless personal communi-\\ncations, Proceedings of the IEEE , September 1994, pp. 1333–1359.\\n2. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 52–53.\\n3. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, p. 172.\\n4. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 53–54.\\n5. ITU-R Recommendations, Attenuation in vegetation , ITU-R P .833-3, Geneva, 2001.\\n6. J. J. Egli, Radio Propagation above 40 MC over irregular terrain, Proceedings of the\\nIRE, October 1957.\\n7. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 156–157.\\n8. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 159–163.\\n9. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 56–60.\\n10. T. S. Rappaport, Wireless Communications, Principles and Practice , 2nd ed.,\\nPrentice-Hall, Upper Saddle River, NJ, 2002, p. 145.\\n11. Irregular Terrain Model (ITM), from the NTIA web site, http://ntiacsd.\\nntia.doc.gov/msam/\\n12. ITU-R Recommendations, Propagation data and prediction methods required for\\nthe design of terrestrial line-of-sight systems , ITU-R P .530-9, Geneva, 2001.\\n13. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, Chapter 4.\\n14. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 77–79.\\n15. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 254–255.\\n16. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, p. 74.', 'metadata': {'source': 'Training_materials/EE 625 Radio Wave Propagation.pdf', 'page': 178}}, page_content='ware is unusual and generally not desirable because the credibility (although\\nnot necessarily the accuracy) of a model is proportional to how widelyaccepted it is.\\nREFERENCES\\n1. H. L. Bertoni, et al., UHF propagation prediction for wireless personal communi-\\ncations, Proceedings of the IEEE , September 1994, pp. 1333–1359.\\n2. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 52–53.\\n3. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, p. 172.\\n4. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 53–54.\\n5. ITU-R Recommendations, Attenuation in vegetation , ITU-R P .833-3, Geneva, 2001.\\n6. J. J. Egli, Radio Propagation above 40 MC over irregular terrain, Proceedings of the\\nIRE, October 1957.\\n7. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 156–157.\\n8. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 159–163.\\n9. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 56–60.\\n10. T. S. Rappaport, Wireless Communications, Principles and Practice , 2nd ed.,\\nPrentice-Hall, Upper Saddle River, NJ, 2002, p. 145.\\n11. Irregular Terrain Model (ITM), from the NTIA web site, http://ntiacsd.\\nntia.doc.gov/msam/\\n12. ITU-R Recommendations, Propagation data and prediction methods required for\\nthe design of terrestrial line-of-sight systems , ITU-R P .530-9, Geneva, 2001.\\n13. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, Chapter 4.\\n14. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, pp. 77–79.\\n15. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\nMA, 2000, pp. 254–255.\\n16. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\n2000, p. 74.', metadata={'source': 'Training_materials/EE 625 Radio Wave Propagation.pdf', 'page': 178})]\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\\n\\nSearch: useful for when you need to answer questions about current events\\nLookup: useful for when you need to answer questions about specific precise information in documents\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Search, Lookup][the input to the action]\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: Final Answer: the final answer to the original input question\\n\\n\\nQuestion: Musician and satirist Allie Goertz wrote a song about the \\\"The Simpsons\\\" character Milhouse, who Matt Groening named after who?\\nThought: The question simplifies to \\\"The Simpsons\\\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\\nAction: Search[Milhouse]\\nObservation: Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\\nThought: The paragraph does not tell who Milhouse is named after, maybe I can look up \\\"named after\\\".\\nAction: Lookup[named after]\\nObservation: (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\\nThought: Final Answer: Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\\n\\n\\nBegin!\\n\\nPrevious conversation history:\\n\\n\\nQuestion: What is the most promising direction for investment for the Vantage Towers?\\nThought: This question requires me to know the current state of the market and the company's financial situation. I will need to use my knowledge of the industry and research on the company's performance.\\nAction: Lookup[Vantage Towers]\\nObservation: [Document(lc_kwargs={'page_content': 'ware is unusual and generally not desirable because the credibility (although\\\\nnot necessarily the accuracy) of a model is proportional to how widelyaccepted it is.\\\\nREFERENCES\\\\n1. H. L. Bertoni, et al., UHF propagation prediction for wireless personal communi-\\\\ncations, Proceedings of the IEEE , September 1994, pp. 1333–1359.\\\\n2. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 52–53.\\\\n3. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, p. 172.\\\\n4. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 53–54.\\\\n5. ITU-R Recommendations, Attenuation in vegetation , ITU-R P .833-3, Geneva, 2001.\\\\n6. J. J. Egli, Radio Propagation above 40 MC over irregular terrain, Proceedings of the\\\\nIRE, October 1957.\\\\n7. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 156–157.\\\\n8. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 159–163.\\\\n9. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 56–60.\\\\n10. T. S. Rappaport, Wireless Communications, Principles and Practice , 2nd ed.,\\\\nPrentice-Hall, Upper Saddle River, NJ, 2002, p. 145.\\\\n11. Irregular Terrain Model (ITM), from the NTIA web site, http://ntiacsd.\\\\nntia.doc.gov/msam/\\\\n12. ITU-R Recommendations, Propagation data and prediction methods required for\\\\nthe design of terrestrial line-of-sight systems , ITU-R P .530-9, Geneva, 2001.\\\\n13. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, Chapter 4.\\\\n14. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 77–79.\\\\n15. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 254–255.\\\\n16. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, p. 74.', 'metadata': {'source': 'Training_materials/EE 625 Radio Wave Propagation.pdf', 'page': 178}}, page_content='ware is unusual and generally not desirable because the credibility (although\\\\nnot necessarily the accuracy) of a model is proportional to how widelyaccepted it is.\\\\nREFERENCES\\\\n1. H. L. Bertoni, et al., UHF propagation prediction for wireless personal communi-\\\\ncations, Proceedings of the IEEE , September 1994, pp. 1333–1359.\\\\n2. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 52–53.\\\\n3. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, p. 172.\\\\n4. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 53–54.\\\\n5. ITU-R Recommendations, Attenuation in vegetation , ITU-R P .833-3, Geneva, 2001.\\\\n6. J. J. Egli, Radio Propagation above 40 MC over irregular terrain, Proceedings of the\\\\nIRE, October 1957.\\\\n7. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 156–157.\\\\n8. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 159–163.\\\\n9. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 56–60.\\\\n10. T. S. Rappaport, Wireless Communications, Principles and Practice , 2nd ed.,\\\\nPrentice-Hall, Upper Saddle River, NJ, 2002, p. 145.\\\\n11. Irregular Terrain Model (ITM), from the NTIA web site, http://ntiacsd.\\\\nntia.doc.gov/msam/\\\\n12. ITU-R Recommendations, Propagation data and prediction methods required for\\\\nthe design of terrestrial line-of-sight systems , ITU-R P .530-9, Geneva, 2001.\\\\n13. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, Chapter 4.\\\\n14. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, pp. 77–79.\\\\n15. N. Blaunstein, Radio Propagation in Cellular Networks , Artech House, Norwood,\\\\nMA, 2000, pp. 254–255.\\\\n16. J. D. Parsons, The Mobile Radio Propagation Channel , 2nd ed., Wiley, West Sussex,\\\\n2000, p. 74.', metadata={'source': 'Training_materials/EE 625 Radio Wave Propagation.pdf', 'page': 178})]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:HuggingFacePipeline] [41.46s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\\nAction: \",\n",
      "        \"generation_info\": null\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [41.46s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"\\nAction: \"\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [97.84s] Chain run errored with error:\n",
      "\u001b[0m\"ValueError('not enough values to unpack (expected 2, got 1)')\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the most promising direction for investment for the Vantage Towers?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:258\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:147\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    148\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    149\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    150\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    151\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/chains/base.py:141\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    135\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    136\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    137\u001b[0m     inputs,\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    146\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:957\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 957\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m    966\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    967\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:762\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 762\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/gpt/lib/python3.10/site-packages/langchain/agents/agent.py:345\u001b[0m, in \u001b[0;36mLLMSingleActionAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    340\u001b[0m     intermediate_steps\u001b[38;5;241m=\u001b[39mintermediate_steps,\n\u001b[1;32m    341\u001b[0m     stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop,\n\u001b[1;32m    342\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    344\u001b[0m )\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 100\u001b[0m, in \u001b[0;36mCustomOutputParser.parse\u001b[0;34m(self, llm_output)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AgentFinish(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;66;03m# Return values is generally always a dictionary with a single `output` key\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# It is not recommended to try anything else at the moment :)\u001b[39;00m\n\u001b[1;32m     86\u001b[0m         return_values\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()},\n\u001b[1;32m     87\u001b[0m         log\u001b[38;5;241m=\u001b[39mllm_output,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Parse out the action and action input\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m#regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#match = re.search(regex, llm_output, re.DOTALL)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Return the action and action input\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# answer = AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m action, tool_input \u001b[38;5;241m=\u001b[39m llm_output\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAction: \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    101\u001b[0m answer \u001b[38;5;241m=\u001b[39m AgentAction(tool\u001b[38;5;241m=\u001b[39maction, tool_input\u001b[38;5;241m=\u001b[39mtool_input, log\u001b[38;5;241m=\u001b[39mllm_output)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "agent_executor.run(\"What is the most promising direction for investment for the Vantage Towers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AgentType.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent_chain = initialize_agent(\n",
    "#     tools,\n",
    "#     llm,\n",
    "#     agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "#     verbose=True,\n",
    "#     memory=ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent_chain.run(\"How can I treat a spained ankle?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# template = \"\"\"Answer the following questions as best you can, but speaking as compasionate specialist in cell towers and all related fields. You have access to the following tools:\n",
    "#\n",
    "# {tools}\n",
    "#\n",
    "# Use the following format:\n",
    "#\n",
    "# Question: the input question you must answer\n",
    "# Thought: you should always think about what to do\n",
    "# Action: the action to take, should be one of [{tool_names}]\n",
    "# Action Input: the input to the action\n",
    "# Observation: the result of the action\n",
    "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "# Thought: I now know the final answer\n",
    "# Final Answer: the final answer to the original input question\n",
    "#\n",
    "# Begin! Remember to answer as a compasionate specialist in cell towers and all related fields when giving your final answer.\n",
    "#\n",
    "# Previous conversation history:\n",
    "# {history}\n",
    "#\n",
    "# New question: {input}\n",
    "# {agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
